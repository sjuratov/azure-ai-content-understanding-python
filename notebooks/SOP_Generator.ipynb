{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Content from Your File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrate you can use Content Understanding API to extract semantic content from multimodal files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "1. Ensure Azure AI service is configured following [steps](../README.md#configure-azure-ai-service-resource)\n",
    "2. Install the required packages to run the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Azure AI Content Understanding Client\n",
    "\n",
    "> The [AzureContentUnderstandingClient](../python/content_understanding_client.py) is a utility class containing functions to interact with the Content Understanding API. Before the official release of the Content Understanding SDK, it can be regarded as a lightweight SDK.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:azure.identity._credentials.environment:No environment configuration found.\n",
      "INFO:azure.identity._credentials.managed_identity:ManagedIdentityCredential will use IMDS\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:Request URL: 'http://169.254.169.254/metadata/identity/oauth2/token?api-version=REDACTED&resource=REDACTED'\n",
      "Request method: 'GET'\n",
      "Request headers:\n",
      "    'User-Agent': 'azsdk-python-identity/1.19.0 Python/3.13.1 (macOS-15.3-arm64-arm-64bit-Mach-O)'\n",
      "No body was attached to the request\n",
      "INFO:azure.identity._credentials.chained:DefaultAzureCredential acquired a token from AzureCliCredential\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "AZURE_AI_ENDPOINT = os.getenv(\"AZURE_AI_ENDPOINT\")\n",
    "AZURE_AI_API_VERSION = os.getenv(\"AZURE_AI_API_VERSION\", \"2024-12-01-preview\")\n",
    "\n",
    "# Add the parent directory to the path to use shared modules\n",
    "parent_dir = Path(Path.cwd()).parent\n",
    "sys.path.append(str(parent_dir))\n",
    "from python.content_understanding_client import AzureContentUnderstandingClient\n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "token_provider = get_bearer_token_provider(credential, \"https://cognitiveservices.azure.com/.default\")\n",
    "\n",
    "client = AzureContentUnderstandingClient(\n",
    "    endpoint=AZURE_AI_ENDPOINT,\n",
    "    api_version=AZURE_AI_API_VERSION,\n",
    "    token_provider=token_provider,\n",
    "    x_ms_useragent=\"azure-ai-content-understanding-python/content_extraction\", # This header is used for sample usage telemetry, please comment out this line if you want to opt out.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Content\n",
    "Video output provides detailed information about audiovisual content, specifically video shots. Here are the key features it offers:\n",
    "\n",
    "1. Shot Information: Each shot is defined by a start and end time, along with a unique identifier. For example, Shot 0:0.0 to 0:2.800 includes a transcript and key frames.\n",
    "1. Transcript: The API includes a transcript of the audio, formatted in WEBVTT, which allows for easy synchronization with the video. It captures spoken content and specifies the timing of the dialogue.\n",
    "1. Key Frames: It provides a series of key frames (images) that represent important moments in the video shot, allowing users to visualize the content at specific timestamps.\n",
    "1. Description: Each shot is accompanied by a description, providing context about the visuals presented. This helps in understanding the scene or subject matter without watching the video.\n",
    "1. Audio Visual Metadata: Details about the video such as dimensions (width and height), type (audiovisual), and the presence of key frame timestamps are included.\n",
    "1. Transcript Phrases: The output includes specific phrases from the transcript, along with timing and speaker information, enhancing the usability for applications like closed captioning or search functionalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:python.content_understanding_client:Analyzer content-video-sample-8d1242a6-cbe0-4414-af65-b53fee8ed15c create request accepted.\n",
      "INFO:python.content_understanding_client:Request result is ready after 0.00 seconds.\n",
      "INFO:python.content_understanding_client:Analyzing file ../data/FlightSimulator.mp4 with analyzer: content-video-sample-8d1242a6-cbe0-4414-af65-b53fee8ed15c\n",
      "INFO:python.content_understanding_client:Request cdb3b11c-7df8-4e48-92ed-854478992b4e in progress ...\n",
      "INFO:python.content_understanding_client:Request cdb3b11c-7df8-4e48-92ed-854478992b4e in progress ...\n",
      "INFO:python.content_understanding_client:Request cdb3b11c-7df8-4e48-92ed-854478992b4e in progress ...\n",
      "INFO:python.content_understanding_client:Request cdb3b11c-7df8-4e48-92ed-854478992b4e in progress ...\n",
      "INFO:python.content_understanding_client:Request cdb3b11c-7df8-4e48-92ed-854478992b4e in progress ...\n",
      "INFO:python.content_understanding_client:Request cdb3b11c-7df8-4e48-92ed-854478992b4e in progress ...\n",
      "INFO:python.content_understanding_client:Request cdb3b11c-7df8-4e48-92ed-854478992b4e in progress ...\n",
      "INFO:python.content_understanding_client:Request result is ready after 17.32 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cdb3b11c-7df8-4e48-92ed-854478992b4e\",\n",
      "  \"status\": \"Succeeded\",\n",
      "  \"result\": {\n",
      "    \"analyzerId\": \"content-video-sample-8d1242a6-cbe0-4414-af65-b53fee8ed15c\",\n",
      "    \"apiVersion\": \"2024-12-01-preview\",\n",
      "    \"createdAt\": \"2025-03-08T08:40:35Z\",\n",
      "    \"warnings\": [],\n",
      "    \"contents\": [\n",
      "      {\n",
      "        \"markdown\": \"# Shot 00:00.000 => 00:01.467\\n## Transcript\\n```\\nWEBVTT\\n\\n00:01.400 --> 00:06.560\\n<v Speaker>When it comes to the neural TTS, in order to get a good voice, it's better to have good data.\\n```\\n## Key Frames\\n- 00:00.726 ![](keyFrame.726.jpg)\",\n",
      "        \"fields\": {},\n",
      "        \"kind\": \"audioVisual\",\n",
      "        \"startTimeMs\": 0,\n",
      "        \"endTimeMs\": 1467,\n",
      "        \"width\": 1080,\n",
      "        \"height\": 608,\n",
      "        \"KeyFrameTimesMs\": [\n",
      "          726\n",
      "        ],\n",
      "        \"transcriptPhrases\": [\n",
      "          {\n",
      "            \"speaker\": \"speaker\",\n",
      "            \"startTimeMs\": 1400,\n",
      "            \"endTimeMs\": 6560,\n",
      "            \"text\": \"When it comes to the neural TTS, in order to get a good voice, it's better to have good data.\",\n",
      "            \"confidence\": 1,\n",
      "            \"words\": [],\n",
      "            \"locale\": \"en-US\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"markdown\": \"# Shot 00:01.467 => 00:03.233\\n## Transcript\\n```\\nWEBVTT\\n\\n00:01.400 --> 00:06.560\\n<v Speaker>When it comes to the neural TTS, in order to get a good voice, it's better to have good data.\\n```\\n## Key Frames\\n- 00:02.046 ![](keyFrame.2046.jpg)\\n- 00:02.640 ![](keyFrame.2640.jpg)\",\n",
      "        \"fields\": {},\n",
      "        \"kind\": \"audioVisual\",\n",
      "        \"startTimeMs\": 1467,\n",
      "        \"endTimeMs\": 3233,\n",
      "        \"width\": 1080,\n",
      "        \"height\": 608,\n",
      "        \"KeyFrameTimesMs\": [\n",
      "          2046,\n",
      "          2640\n",
      "        ],\n",
      "        \"transcriptPhrases\": [\n",
      "          {\n",
      "            \"speaker\": \"speaker\",\n",
      "            \"startTimeMs\": 1400,\n",
      "            \"endTimeMs\": 6560,\n",
      "            \"text\": \"When it comes to the neural TTS, in order to get a good voice, it's better to have good data.\",\n",
      "            \"confidence\": 1,\n",
      "            \"words\": [],\n",
      "            \"locale\": \"en-US\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"markdown\": \"# Shot 00:03.233 => 00:07.367\\n## Transcript\\n```\\nWEBVTT\\n\\n00:01.400 --> 00:06.560\\n<v Speaker>When it comes to the neural TTS, in order to get a good voice, it's better to have good data.\\n```\\n## Key Frames\\n- 00:04.059 ![](keyFrame.4059.jpg)\\n- 00:04.884 ![](keyFrame.4884.jpg)\\n- 00:05.709 ![](keyFrame.5709.jpg)\\n- 00:06.534 ![](keyFrame.6534.jpg)\",\n",
      "        \"fields\": {},\n",
      "        \"kind\": \"audioVisual\",\n",
      "        \"startTimeMs\": 3233,\n",
      "        \"endTimeMs\": 7367,\n",
      "        \"width\": 1080,\n",
      "        \"height\": 608,\n",
      "        \"KeyFrameTimesMs\": [\n",
      "          4059,\n",
      "          4884,\n",
      "          5709,\n",
      "          6534\n",
      "        ],\n",
      "        \"transcriptPhrases\": [\n",
      "          {\n",
      "            \"speaker\": \"speaker\",\n",
      "            \"startTimeMs\": 1400,\n",
      "            \"endTimeMs\": 6560,\n",
      "            \"text\": \"When it comes to the neural TTS, in order to get a good voice, it's better to have good data.\",\n",
      "            \"confidence\": 1,\n",
      "            \"words\": [],\n",
      "            \"locale\": \"en-US\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"markdown\": \"# Shot 00:07.367 => 00:08.200\\n## Transcript\\n```\\nWEBVTT\\n\\n00:07.600 --> 00:13.320\\n<v Speaker>To achieve that, we build a universal TTS model based on 3,000 hours of data.\\n```\\n## Key Frames\\n- 00:07.788 ![](keyFrame.7788.jpg)\",\n",
      "        \"fields\": {},\n",
      "        \"kind\": \"audioVisual\",\n",
      "        \"startTimeMs\": 7367,\n",
      "        \"endTimeMs\": 8200,\n",
      "        \"width\": 1080,\n",
      "        \"height\": 608,\n",
      "        \"KeyFrameTimesMs\": [\n",
      "          7788\n",
      "        ],\n",
      "        \"transcriptPhrases\": [\n",
      "          {\n",
      "            \"speaker\": \"speaker\",\n",
      "            \"startTimeMs\": 7600,\n",
      "            \"endTimeMs\": 13320,\n",
      "            \"text\": \"To achieve that, we build a universal TTS model based on 3,000 hours of data.\",\n",
      "            \"confidence\": 1,\n",
      "            \"words\": [],\n",
      "            \"locale\": \"en-US\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"markdown\": \"# Shot 00:08.200 => 00:11.367\\n## Transcript\\n```\\nWEBVTT\\n\\n00:07.600 --> 00:13.320\\n<v Speaker>To achieve that, we build a universal TTS model based on 3,000 hours of data.\\n```\\n## Key Frames\\n- 00:08.976 ![](keyFrame.8976.jpg)\\n- 00:09.768 ![](keyFrame.9768.jpg)\\n- 00:10.560 ![](keyFrame.10560.jpg)\",\n",
      "        \"fields\": {},\n",
      "        \"kind\": \"audioVisual\",\n",
      "        \"startTimeMs\": 8200,\n",
      "        \"endTimeMs\": 11367,\n",
      "        \"width\": 1080,\n",
      "        \"height\": 608,\n",
      "        \"KeyFrameTimesMs\": [\n",
      "          8976,\n",
      "          9768,\n",
      "          10560\n",
      "        ],\n",
      "        \"transcriptPhrases\": [\n",
      "          {\n",
      "            \"speaker\": \"speaker\",\n",
      "            \"startTimeMs\": 7600,\n",
      "            \"endTimeMs\": 13320,\n",
      "            \"text\": \"To achieve that, we build a universal TTS model based on 3,000 hours of data.\",\n",
      "            \"confidence\": 1,\n",
      "            \"words\": [],\n",
      "            \"locale\": \"en-US\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"markdown\": \"# Shot 00:11.367 => 00:13.567\\n## Transcript\\n```\\nWEBVTT\\n\\n00:07.600 --> 00:13.320\\n<v Speaker>To achieve that, we build a universal TTS model based on 3,000 hours of data.\\n00:13.440 --> 00:23.640\\n<v Speaker>We actually accumulated tons of the data so that this universal model is able to capture the nuance of the audio and generate a more natural voice for the algorithm.\\n```\\n## Key Frames\\n- 00:12.078 ![](keyFrame.12078.jpg)\\n- 00:12.804 ![](keyFrame.12804.jpg)\",\n",
      "        \"fields\": {},\n",
      "        \"kind\": \"audioVisual\",\n",
      "        \"startTimeMs\": 11367,\n",
      "        \"endTimeMs\": 13567,\n",
      "        \"width\": 1080,\n",
      "        \"height\": 608,\n",
      "        \"KeyFrameTimesMs\": [\n",
      "          12078,\n",
      "          12804\n",
      "        ],\n",
      "        \"transcriptPhrases\": [\n",
      "          {\n",
      "            \"speaker\": \"speaker\",\n",
      "            \"startTimeMs\": 7600,\n",
      "            \"endTimeMs\": 13320,\n",
      "            \"text\": \"To achieve that, we build a universal TTS model based on 3,000 hours of data.\",\n",
      "            \"confidence\": 1,\n",
      "            \"words\": [],\n",
      "            \"locale\": \"en-US\"\n",
      "          },\n",
      "          {\n",
      "            \"speaker\": \"speaker\",\n",
      "            \"startTimeMs\": 13440,\n",
      "            \"endTimeMs\": 23640,\n",
      "            \"text\": \"We actually accumulated tons of the data so that this universal model is able to capture the nuance of the audio and generate a more natural voice for the algorithm.\",\n",
      "            \"confidence\": 1,\n",
      "            \"words\": [],\n",
      "            \"locale\": \"en-US\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"markdown\": \"# Shot 00:13.567 => 00:16.100\\n## Transcript\\n```\\nWEBVTT\\n\\n00:13.440 --> 00:23.640\\n<v Speaker>We actually accumulated tons of the data so that this universal model is able to capture the nuance of the audio and generate a more natural voice for the algorithm.\\n```\\n## Key Frames\\n- 00:14.190 ![](keyFrame.14190.jpg)\\n- 00:14.817 ![](keyFrame.14817.jpg)\\n- 00:15.444 ![](keyFrame.15444.jpg)\",\n",
      "        \"fields\": {},\n",
      "        \"kind\": \"audioVisual\",\n",
      "        \"startTimeMs\": 13567,\n",
      "        \"endTimeMs\": 16100,\n",
      "        \"width\": 1080,\n",
      "        \"height\": 608,\n",
      "        \"KeyFrameTimesMs\": [\n",
      "          14190,\n",
      "          14817,\n",
      "          15444\n",
      "        ],\n",
      "        \"transcriptPhrases\": [\n",
      "          {\n",
      "            \"speaker\": \"speaker\",\n",
      "            \"startTimeMs\": 13440,\n",
      "            \"endTimeMs\": 23640,\n",
      "            \"text\": \"We actually accumulated tons of the data so that this universal model is able to capture the nuance of the audio and generate a more natural voice for the algorithm.\",\n",
      "            \"confidence\": 1,\n",
      "            \"words\": [],\n",
      "            \"locale\": \"en-US\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"markdown\": \"# Shot 00:16.100 => 00:19.433\\n## Transcript\\n```\\nWEBVTT\\n\\n00:13.440 --> 00:23.640\\n<v Speaker>We actually accumulated tons of the data so that this universal model is able to capture the nuance of the audio and generate a more natural voice for the algorithm.\\n```\\n## Key Frames\\n- 00:16.929 ![](keyFrame.16929.jpg)\\n- 00:17.754 ![](keyFrame.17754.jpg)\\n- 00:18.579 ![](keyFrame.18579.jpg)\",\n",
      "        \"fields\": {},\n",
      "        \"kind\": \"audioVisual\",\n",
      "        \"startTimeMs\": 16100,\n",
      "        \"endTimeMs\": 19433,\n",
      "        \"width\": 1080,\n",
      "        \"height\": 608,\n",
      "        \"KeyFrameTimesMs\": [\n",
      "          16929,\n",
      "          17754,\n",
      "          18579\n",
      "        ],\n",
      "        \"transcriptPhrases\": [\n",
      "          {\n",
      "            \"speaker\": \"speaker\",\n",
      "            \"startTimeMs\": 13440,\n",
      "            \"endTimeMs\": 23640,\n",
      "            \"text\": \"We actually accumulated tons of the data so that this universal model is able to capture the nuance of the audio and generate a more natural voice for the algorithm.\",\n",
      "            \"confidence\": 1,\n",
      "            \"words\": [],\n",
      "            \"locale\": \"en-US\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"markdown\": \"# Shot 00:19.433 => 00:23.967\\n## Transcript\\n```\\nWEBVTT\\n\\n00:13.440 --> 00:23.640\\n<v Speaker>We actually accumulated tons of the data so that this universal model is able to capture the nuance of the audio and generate a more natural voice for the algorithm.\\n```\\n## Key Frames\\n- 00:20.196 ![](keyFrame.20196.jpg)\\n- 00:20.955 ![](keyFrame.20955.jpg)\\n- 00:21.714 ![](keyFrame.21714.jpg)\\n- 00:22.473 ![](keyFrame.22473.jpg)\\n- 00:23.232 ![](keyFrame.23232.jpg)\",\n",
      "        \"fields\": {},\n",
      "        \"kind\": \"audioVisual\",\n",
      "        \"startTimeMs\": 19433,\n",
      "        \"endTimeMs\": 23967,\n",
      "        \"width\": 1080,\n",
      "        \"height\": 608,\n",
      "        \"KeyFrameTimesMs\": [\n",
      "          20196,\n",
      "          20955,\n",
      "          21714,\n",
      "          22473,\n",
      "          23232\n",
      "        ],\n",
      "        \"transcriptPhrases\": [\n",
      "          {\n",
      "            \"speaker\": \"speaker\",\n",
      "            \"startTimeMs\": 13440,\n",
      "            \"endTimeMs\": 23640,\n",
      "            \"text\": \"We actually accumulated tons of the data so that this universal model is able to capture the nuance of the audio and generate a more natural voice for the algorithm.\",\n",
      "            \"confidence\": 1,\n",
      "            \"words\": [],\n",
      "            \"locale\": \"en-US\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"markdown\": \"# Shot 00:23.967 => 00:30.033\\n## Transcript\\n```\\nWEBVTT\\n\\n00:24.080 --> 00:29.120\\n<v Speaker>What we liked about cognitive services offerings were that they had a much higher fidelity.\\n00:29.600 --> 00:32.880\\n<v Speaker>And they sounded a lot more like an actual human voice.\\n```\\n## Key Frames\\n- 00:24.816 ![](keyFrame.24816.jpg)\\n- 00:25.674 ![](keyFrame.25674.jpg)\\n- 00:26.532 ![](keyFrame.26532.jpg)\\n- 00:27.390 ![](keyFrame.27390.jpg)\\n- 00:28.248 ![](keyFrame.28248.jpg)\\n- 00:29.106 ![](keyFrame.29106.jpg)\",\n",
      "        \"fields\": {},\n",
      "        \"kind\": \"audioVisual\",\n",
      "        \"startTimeMs\": 23967,\n",
      "        \"endTimeMs\": 30033,\n",
      "        \"width\": 1080,\n",
      "        \"height\": 608,\n",
      "        \"KeyFrameTimesMs\": [\n",
      "          24816,\n",
      "          25674,\n",
      "          26532,\n",
      "          27390,\n",
      "          28248,\n",
      "          29106\n",
      "        ],\n",
      "        \"transcriptPhrases\": [\n",
      "          {\n",
      "            \"speaker\": \"speaker\",\n",
      "            \"startTimeMs\": 24080,\n",
      "            \"endTimeMs\": 29120,\n",
      "            \"text\": \"What we liked about cognitive services offerings were that they had a much higher fidelity.\",\n",
      "            \"confidence\": 1,\n",
      "            \"words\": [],\n",
      "            \"locale\": \"en-US\"\n",
      "          },\n",
      "          {\n",
      "            \"speaker\": \"speaker\",\n",
      "            \"startTimeMs\": 29600,\n",
      "            \"endTimeMs\": 32880,\n",
      "            \"text\": \"And they sounded a lot more like an actual human voice.\",\n",
      "            \"confidence\": 1,\n",
      "            \"words\": [],\n",
      "            \"locale\": \"en-US\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"markdown\": \"# Shot 00:30.033 => 00:33.200\\n## Transcript\\n```\\nWEBVTT\\n\\n00:29.600 --> 00:32.880\\n<v Speaker>And they sounded a lot more like an actual human voice.\\n```\\n## Key Frames\\n- 00:30.822 ![](keyFrame.30822.jpg)\\n- 00:31.614 ![](keyFrame.31614.jpg)\\n- 00:32.406 ![](keyFrame.32406.jpg)\",\n",
      "        \"fields\": {},\n",
      "        \"kind\": \"audioVisual\",\n",
      "        \"startTimeMs\": 30033,\n",
      "        \"endTimeMs\": 33200,\n",
      "        \"width\": 1080,\n",
      "        \"height\": 608,\n",
      "        \"KeyFrameTimesMs\": [\n",
      "          30822,\n",
      "          31614,\n",
      "          32406\n",
      "        ],\n",
      "        \"transcriptPhrases\": [\n",
      "          {\n",
      "            \"speaker\": \"speaker\",\n",
      "            \"startTimeMs\": 29600,\n",
      "            \"endTimeMs\": 32880,\n",
      "            \"text\": \"And they sounded a lot more like an actual human voice.\",\n",
      "            \"confidence\": 1,\n",
      "            \"words\": [],\n",
      "            \"locale\": \"en-US\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"markdown\": \"# Shot 00:33.200 => 00:35.267\\n## Transcript\\n```\\nWEBVTT\\n\\n00:33.720 --> 00:37.080\\n<v Speaker>Orlando ground 9555 requesting the end of pushback.\\n```\\n## Key Frames\\n- 00:33.891 ![](keyFrame.33891.jpg)\\n- 00:34.584 ![](keyFrame.34584.jpg)\",\n",
      "        \"fields\": {},\n",
      "        \"kind\": \"audioVisual\",\n",
      "        \"startTimeMs\": 33200,\n",
      "        \"endTimeMs\": 35267,\n",
      "        \"width\": 1080,\n",
      "        \"height\": 608,\n",
      "        \"KeyFrameTimesMs\": [\n",
      "          33891,\n",
      "          34584\n",
      "        ],\n",
      "        \"transcriptPhrases\": [\n",
      "          {\n",
      "            \"speaker\": \"speaker\",\n",
      "            \"startTimeMs\": 33720,\n",
      "            \"endTimeMs\": 37080,\n",
      "            \"text\": \"Orlando ground 9555 requesting the end of pushback.\",\n",
      "            \"confidence\": 1,\n",
      "            \"words\": [],\n",
      "            \"locale\": \"en-US\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"markdown\": \"# Shot 00:35.267 => 00:37.700\\n## Transcript\\n```\\nWEBVTT\\n\\n00:33.720 --> 00:37.080\\n<v Speaker>Orlando ground 9555 requesting the end of pushback.\\n```\\n## Key Frames\\n- 00:36.069 ![](keyFrame.36069.jpg)\\n- 00:36.861 ![](keyFrame.36861.jpg)\",\n",
      "        \"fields\": {},\n",
      "        \"kind\": \"audioVisual\",\n",
      "        \"startTimeMs\": 35267,\n",
      "        \"endTimeMs\": 37700,\n",
      "        \"width\": 1080,\n",
      "        \"height\": 608,\n",
      "        \"KeyFrameTimesMs\": [\n",
      "          36069,\n",
      "          36861\n",
      "        ],\n",
      "        \"transcriptPhrases\": [\n",
      "          {\n",
      "            \"speaker\": \"speaker\",\n",
      "            \"startTimeMs\": 33720,\n",
      "            \"endTimeMs\": 37080,\n",
      "            \"text\": \"Orlando ground 9555 requesting the end of pushback.\",\n",
      "            \"confidence\": 1,\n",
      "            \"words\": [],\n",
      "            \"locale\": \"en-US\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"markdown\": \"# Shot 00:37.700 => 00:39.200\\n## Transcript\\n```\\nWEBVTT\\n\\n00:38.800 --> 00:41.160\\n<v Speaker>9555 request to end pushback received.\\n```\\n## Key Frames\\n- 00:38.181 ![](keyFrame.38181.jpg)\\n- 00:38.676 ![](keyFrame.38676.jpg)\",\n",
      "        \"fields\": {},\n",
      "        \"kind\": \"audioVisual\",\n",
      "        \"startTimeMs\": 37700,\n",
      "        \"endTimeMs\": 39200,\n",
      "        \"width\": 1080,\n",
      "        \"height\": 608,\n",
      "        \"KeyFrameTimesMs\": [\n",
      "          38181,\n",
      "          38676\n",
      "        ],\n",
      "        \"transcriptPhrases\": [\n",
      "          {\n",
      "            \"speaker\": \"speaker\",\n",
      "            \"startTimeMs\": 38800,\n",
      "            \"endTimeMs\": 41160,\n",
      "            \"text\": \"9555 request to end pushback received.\",\n",
      "            \"confidence\": 1,\n",
      "            \"words\": [],\n",
      "            \"locale\": \"en-US\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"markdown\": \"# Shot 00:39.200 => 00:42.033\\n## Transcript\\n```\\nWEBVTT\\n\\n00:38.800 --> 00:41.160\\n<v Speaker>9555 request to end pushback received.\\n```\\n## Key Frames\\n- 00:39.897 ![](keyFrame.39897.jpg)\\n- 00:40.590 ![](keyFrame.40590.jpg)\\n- 00:41.283 ![](keyFrame.41283.jpg)\",\n",
      "        \"fields\": {},\n",
      "        \"kind\": \"audioVisual\",\n",
      "        \"startTimeMs\": 39200,\n",
      "        \"endTimeMs\": 42033,\n",
      "        \"width\": 1080,\n",
      "        \"height\": 608,\n",
      "        \"KeyFrameTimesMs\": [\n",
      "          39897,\n",
      "          40590,\n",
      "          41283\n",
      "        ],\n",
      "        \"transcriptPhrases\": [\n",
      "          {\n",
      "            \"speaker\": \"speaker\",\n",
      "            \"startTimeMs\": 38800,\n",
      "            \"endTimeMs\": 41160,\n",
      "            \"text\": \"9555 request to end pushback received.\",\n",
      "            \"confidence\": 1,\n",
      "            \"words\": [],\n",
      "            \"locale\": \"en-US\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"markdown\": \"# Shot 00:42.033 => 00:43.866\\n## Transcript\\n```\\nWEBVTT\\n\\n```\\n## Key Frames\\n- 00:42.636 ![](keyFrame.42636.jpg)\\n- 00:43.230 ![](keyFrame.43230.jpg)\",\n",
      "        \"fields\": {},\n",
      "        \"kind\": \"audioVisual\",\n",
      "        \"startTimeMs\": 42033,\n",
      "        \"endTimeMs\": 43866,\n",
      "        \"width\": 1080,\n",
      "        \"height\": 608,\n",
      "        \"KeyFrameTimesMs\": [\n",
      "          42636,\n",
      "          43230\n",
      "        ],\n",
      "        \"transcriptPhrases\": []\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "ANALYZER_ID = \"content-video-sample-\" + str(uuid.uuid4())\n",
    "ANALYZER_TEMPLATE_FILE = '../analyzer_templates/content_video.json'\n",
    "ANALYZER_SAMPLE_FILE = '../data/FlightSimulator.mp4'\n",
    "\n",
    "# Create analyzer\n",
    "response = client.begin_create_analyzer(ANALYZER_ID, analyzer_template_path=ANALYZER_TEMPLATE_FILE)\n",
    "result = client.poll_result(response)\n",
    "\n",
    "# Analyzer file\n",
    "response = client.begin_analyze(ANALYZER_ID, file_location=ANALYZER_SAMPLE_FILE)\n",
    "result = client.poll_result(response)\n",
    "\n",
    "print(json.dumps(result, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             markdown fields         kind  \\\n",
      "0   # Shot 00:00.000 => 00:01.467\\n## Transcript\\n...     {}  audioVisual   \n",
      "1   # Shot 00:01.467 => 00:03.233\\n## Transcript\\n...     {}  audioVisual   \n",
      "2   # Shot 00:03.233 => 00:07.367\\n## Transcript\\n...     {}  audioVisual   \n",
      "3   # Shot 00:07.367 => 00:08.200\\n## Transcript\\n...     {}  audioVisual   \n",
      "4   # Shot 00:08.200 => 00:11.367\\n## Transcript\\n...     {}  audioVisual   \n",
      "5   # Shot 00:11.367 => 00:13.567\\n## Transcript\\n...     {}  audioVisual   \n",
      "6   # Shot 00:13.567 => 00:16.100\\n## Transcript\\n...     {}  audioVisual   \n",
      "7   # Shot 00:16.100 => 00:19.433\\n## Transcript\\n...     {}  audioVisual   \n",
      "8   # Shot 00:19.433 => 00:23.967\\n## Transcript\\n...     {}  audioVisual   \n",
      "9   # Shot 00:23.967 => 00:30.033\\n## Transcript\\n...     {}  audioVisual   \n",
      "10  # Shot 00:30.033 => 00:33.200\\n## Transcript\\n...     {}  audioVisual   \n",
      "11  # Shot 00:33.200 => 00:35.267\\n## Transcript\\n...     {}  audioVisual   \n",
      "12  # Shot 00:35.267 => 00:37.700\\n## Transcript\\n...     {}  audioVisual   \n",
      "13  # Shot 00:37.700 => 00:39.200\\n## Transcript\\n...     {}  audioVisual   \n",
      "14  # Shot 00:39.200 => 00:42.033\\n## Transcript\\n...     {}  audioVisual   \n",
      "15  # Shot 00:42.033 => 00:43.866\\n## Transcript\\n...     {}  audioVisual   \n",
      "\n",
      "    startTimeMs  endTimeMs  width  height  \\\n",
      "0             0       1467   1080     608   \n",
      "1          1467       3233   1080     608   \n",
      "2          3233       7367   1080     608   \n",
      "3          7367       8200   1080     608   \n",
      "4          8200      11367   1080     608   \n",
      "5         11367      13567   1080     608   \n",
      "6         13567      16100   1080     608   \n",
      "7         16100      19433   1080     608   \n",
      "8         19433      23967   1080     608   \n",
      "9         23967      30033   1080     608   \n",
      "10        30033      33200   1080     608   \n",
      "11        33200      35267   1080     608   \n",
      "12        35267      37700   1080     608   \n",
      "13        37700      39200   1080     608   \n",
      "14        39200      42033   1080     608   \n",
      "15        42033      43866   1080     608   \n",
      "\n",
      "                               KeyFrameTimesMs  \\\n",
      "0                                        [726]   \n",
      "1                                 [2046, 2640]   \n",
      "2                     [4059, 4884, 5709, 6534]   \n",
      "3                                       [7788]   \n",
      "4                          [8976, 9768, 10560]   \n",
      "5                               [12078, 12804]   \n",
      "6                        [14190, 14817, 15444]   \n",
      "7                        [16929, 17754, 18579]   \n",
      "8          [20196, 20955, 21714, 22473, 23232]   \n",
      "9   [24816, 25674, 26532, 27390, 28248, 29106]   \n",
      "10                       [30822, 31614, 32406]   \n",
      "11                              [33891, 34584]   \n",
      "12                              [36069, 36861]   \n",
      "13                              [38181, 38676]   \n",
      "14                       [39897, 40590, 41283]   \n",
      "15                              [42636, 43230]   \n",
      "\n",
      "                                    transcriptPhrases  \n",
      "0   [{'speaker': 'speaker', 'startTimeMs': 1400, '...  \n",
      "1   [{'speaker': 'speaker', 'startTimeMs': 1400, '...  \n",
      "2   [{'speaker': 'speaker', 'startTimeMs': 1400, '...  \n",
      "3   [{'speaker': 'speaker', 'startTimeMs': 7600, '...  \n",
      "4   [{'speaker': 'speaker', 'startTimeMs': 7600, '...  \n",
      "5   [{'speaker': 'speaker', 'startTimeMs': 7600, '...  \n",
      "6   [{'speaker': 'speaker', 'startTimeMs': 13440, ...  \n",
      "7   [{'speaker': 'speaker', 'startTimeMs': 13440, ...  \n",
      "8   [{'speaker': 'speaker', 'startTimeMs': 13440, ...  \n",
      "9   [{'speaker': 'speaker', 'startTimeMs': 24080, ...  \n",
      "10  [{'speaker': 'speaker', 'startTimeMs': 29600, ...  \n",
      "11  [{'speaker': 'speaker', 'startTimeMs': 33720, ...  \n",
      "12  [{'speaker': 'speaker', 'startTimeMs': 33720, ...  \n",
      "13  [{'speaker': 'speaker', 'startTimeMs': 38800, ...  \n",
      "14  [{'speaker': 'speaker', 'startTimeMs': 38800, ...  \n",
      "15                                                 []  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "result_df = pd.DataFrame(result['result']['contents'])\n",
    "\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY_FRAME_DIR = '../data/key_frames'\n",
    "\n",
    "# Crate key frame directory if it does not exist\n",
    "\n",
    "Path(KEY_FRAME_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Delete files in the key frame directory\n",
    "for file in os.listdir(KEY_FRAME_DIR):\n",
    "    os.remove(os.path.join(KEY_FRAME_DIR, file))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Markdown, display\n",
    "\n",
    "# for index, row in result_df.iterrows():\n",
    "#     markdown_str = \"\"\n",
    "#     for col in result_df.columns:\n",
    "#         # Expand arrays for specific columns\n",
    "#         if col in [\"keyFrameTimesMs\", \"transcriptPhrases\"] and isinstance(row[col], list):\n",
    "#             markdown_str += f\"**{col}:**\\n\"\n",
    "#             for i, item in enumerate(row[col]):\n",
    "#                 markdown_str += f\"  - **{i}:** {item}\\n\"\n",
    "#         else:\n",
    "#             markdown_str += f\"**{col}:** {row[col]}\\n\"\n",
    "#     markdown_str += \"\\n---\\n\"  # Markdown horizontal rule as a separator between rows\n",
    "#     display(Markdown(markdown_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming result_df is your pandas DataFrame and each row in 'KeyFrameTimesMs' is a list of integers\n",
    "for times in result_df['KeyFrameTimesMs']:\n",
    "    for time_ms in times:\n",
    "        key_frame = client.get_image_from_analyze_operation(response, f'keyFrame.{time_ms}')\n",
    "        # Process key_frame as needed\n",
    "        file_path = f'{KEY_FRAME_DIR}/keyFrame.{time_ms}.jpg'\n",
    "        with open(file_path, 'wb') as file:\n",
    "            file.write(key_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "markdown",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "fields",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "kind",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "startTimeMs",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "endTimeMs",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "width",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "height",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "KeyFrameTimesMs",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "transcriptPhrases",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "2c7cd333-12af-4e14-bfd2-e1b4213ce116",
       "rows": [
        [
         "0",
         "# Shot 00:00.000 => 00:01.467\n## Transcript\n```\nWEBVTT\n\n00:01.400 --> 00:06.560\n<v Speaker>When it comes to the neural TTS, in order to get a good voice, it's better to have good data.\n```\n## Key Frames\n- 00:00.726 ![](keyFrame.726.jpg)",
         "{}",
         "audioVisual",
         "0",
         "1467",
         "1080",
         "608",
         "[726]",
         "[{'speaker': 'speaker', 'startTimeMs': 1400, 'endTimeMs': 6560, 'text': \"When it comes to the neural TTS, in order to get a good voice, it's better to have good data.\", 'confidence': 1, 'words': [], 'locale': 'en-US'}]"
        ],
        [
         "1",
         "# Shot 00:01.467 => 00:03.233\n## Transcript\n```\nWEBVTT\n\n00:01.400 --> 00:06.560\n<v Speaker>When it comes to the neural TTS, in order to get a good voice, it's better to have good data.\n```\n## Key Frames\n- 00:02.046 ![](keyFrame.2046.jpg)\n- 00:02.640 ![](keyFrame.2640.jpg)",
         "{}",
         "audioVisual",
         "1467",
         "3233",
         "1080",
         "608",
         "[2046, 2640]",
         "[{'speaker': 'speaker', 'startTimeMs': 1400, 'endTimeMs': 6560, 'text': \"When it comes to the neural TTS, in order to get a good voice, it's better to have good data.\", 'confidence': 1, 'words': [], 'locale': 'en-US'}]"
        ],
        [
         "2",
         "# Shot 00:03.233 => 00:07.367\n## Transcript\n```\nWEBVTT\n\n00:01.400 --> 00:06.560\n<v Speaker>When it comes to the neural TTS, in order to get a good voice, it's better to have good data.\n```\n## Key Frames\n- 00:04.059 ![](keyFrame.4059.jpg)\n- 00:04.884 ![](keyFrame.4884.jpg)\n- 00:05.709 ![](keyFrame.5709.jpg)\n- 00:06.534 ![](keyFrame.6534.jpg)",
         "{}",
         "audioVisual",
         "3233",
         "7367",
         "1080",
         "608",
         "[4059, 4884, 5709, 6534]",
         "[{'speaker': 'speaker', 'startTimeMs': 1400, 'endTimeMs': 6560, 'text': \"When it comes to the neural TTS, in order to get a good voice, it's better to have good data.\", 'confidence': 1, 'words': [], 'locale': 'en-US'}]"
        ],
        [
         "3",
         "# Shot 00:07.367 => 00:08.200\n## Transcript\n```\nWEBVTT\n\n00:07.600 --> 00:13.320\n<v Speaker>To achieve that, we build a universal TTS model based on 3,000 hours of data.\n```\n## Key Frames\n- 00:07.788 ![](keyFrame.7788.jpg)",
         "{}",
         "audioVisual",
         "7367",
         "8200",
         "1080",
         "608",
         "[7788]",
         "[{'speaker': 'speaker', 'startTimeMs': 7600, 'endTimeMs': 13320, 'text': 'To achieve that, we build a universal TTS model based on 3,000 hours of data.', 'confidence': 1, 'words': [], 'locale': 'en-US'}]"
        ],
        [
         "4",
         "# Shot 00:08.200 => 00:11.367\n## Transcript\n```\nWEBVTT\n\n00:07.600 --> 00:13.320\n<v Speaker>To achieve that, we build a universal TTS model based on 3,000 hours of data.\n```\n## Key Frames\n- 00:08.976 ![](keyFrame.8976.jpg)\n- 00:09.768 ![](keyFrame.9768.jpg)\n- 00:10.560 ![](keyFrame.10560.jpg)",
         "{}",
         "audioVisual",
         "8200",
         "11367",
         "1080",
         "608",
         "[8976, 9768, 10560]",
         "[{'speaker': 'speaker', 'startTimeMs': 7600, 'endTimeMs': 13320, 'text': 'To achieve that, we build a universal TTS model based on 3,000 hours of data.', 'confidence': 1, 'words': [], 'locale': 'en-US'}]"
        ],
        [
         "5",
         "# Shot 00:11.367 => 00:13.567\n## Transcript\n```\nWEBVTT\n\n00:07.600 --> 00:13.320\n<v Speaker>To achieve that, we build a universal TTS model based on 3,000 hours of data.\n00:13.440 --> 00:23.640\n<v Speaker>We actually accumulated tons of the data so that this universal model is able to capture the nuance of the audio and generate a more natural voice for the algorithm.\n```\n## Key Frames\n- 00:12.078 ![](keyFrame.12078.jpg)\n- 00:12.804 ![](keyFrame.12804.jpg)",
         "{}",
         "audioVisual",
         "11367",
         "13567",
         "1080",
         "608",
         "[12078, 12804]",
         "[{'speaker': 'speaker', 'startTimeMs': 7600, 'endTimeMs': 13320, 'text': 'To achieve that, we build a universal TTS model based on 3,000 hours of data.', 'confidence': 1, 'words': [], 'locale': 'en-US'}, {'speaker': 'speaker', 'startTimeMs': 13440, 'endTimeMs': 23640, 'text': 'We actually accumulated tons of the data so that this universal model is able to capture the nuance of the audio and generate a more natural voice for the algorithm.', 'confidence': 1, 'words': [], 'locale': 'en-US'}]"
        ],
        [
         "6",
         "# Shot 00:13.567 => 00:16.100\n## Transcript\n```\nWEBVTT\n\n00:13.440 --> 00:23.640\n<v Speaker>We actually accumulated tons of the data so that this universal model is able to capture the nuance of the audio and generate a more natural voice for the algorithm.\n```\n## Key Frames\n- 00:14.190 ![](keyFrame.14190.jpg)\n- 00:14.817 ![](keyFrame.14817.jpg)\n- 00:15.444 ![](keyFrame.15444.jpg)",
         "{}",
         "audioVisual",
         "13567",
         "16100",
         "1080",
         "608",
         "[14190, 14817, 15444]",
         "[{'speaker': 'speaker', 'startTimeMs': 13440, 'endTimeMs': 23640, 'text': 'We actually accumulated tons of the data so that this universal model is able to capture the nuance of the audio and generate a more natural voice for the algorithm.', 'confidence': 1, 'words': [], 'locale': 'en-US'}]"
        ],
        [
         "7",
         "# Shot 00:16.100 => 00:19.433\n## Transcript\n```\nWEBVTT\n\n00:13.440 --> 00:23.640\n<v Speaker>We actually accumulated tons of the data so that this universal model is able to capture the nuance of the audio and generate a more natural voice for the algorithm.\n```\n## Key Frames\n- 00:16.929 ![](keyFrame.16929.jpg)\n- 00:17.754 ![](keyFrame.17754.jpg)\n- 00:18.579 ![](keyFrame.18579.jpg)",
         "{}",
         "audioVisual",
         "16100",
         "19433",
         "1080",
         "608",
         "[16929, 17754, 18579]",
         "[{'speaker': 'speaker', 'startTimeMs': 13440, 'endTimeMs': 23640, 'text': 'We actually accumulated tons of the data so that this universal model is able to capture the nuance of the audio and generate a more natural voice for the algorithm.', 'confidence': 1, 'words': [], 'locale': 'en-US'}]"
        ],
        [
         "8",
         "# Shot 00:19.433 => 00:23.967\n## Transcript\n```\nWEBVTT\n\n00:13.440 --> 00:23.640\n<v Speaker>We actually accumulated tons of the data so that this universal model is able to capture the nuance of the audio and generate a more natural voice for the algorithm.\n```\n## Key Frames\n- 00:20.196 ![](keyFrame.20196.jpg)\n- 00:20.955 ![](keyFrame.20955.jpg)\n- 00:21.714 ![](keyFrame.21714.jpg)\n- 00:22.473 ![](keyFrame.22473.jpg)\n- 00:23.232 ![](keyFrame.23232.jpg)",
         "{}",
         "audioVisual",
         "19433",
         "23967",
         "1080",
         "608",
         "[20196, 20955, 21714, 22473, 23232]",
         "[{'speaker': 'speaker', 'startTimeMs': 13440, 'endTimeMs': 23640, 'text': 'We actually accumulated tons of the data so that this universal model is able to capture the nuance of the audio and generate a more natural voice for the algorithm.', 'confidence': 1, 'words': [], 'locale': 'en-US'}]"
        ],
        [
         "9",
         "# Shot 00:23.967 => 00:30.033\n## Transcript\n```\nWEBVTT\n\n00:24.080 --> 00:29.120\n<v Speaker>What we liked about cognitive services offerings were that they had a much higher fidelity.\n00:29.600 --> 00:32.880\n<v Speaker>And they sounded a lot more like an actual human voice.\n```\n## Key Frames\n- 00:24.816 ![](keyFrame.24816.jpg)\n- 00:25.674 ![](keyFrame.25674.jpg)\n- 00:26.532 ![](keyFrame.26532.jpg)\n- 00:27.390 ![](keyFrame.27390.jpg)\n- 00:28.248 ![](keyFrame.28248.jpg)\n- 00:29.106 ![](keyFrame.29106.jpg)",
         "{}",
         "audioVisual",
         "23967",
         "30033",
         "1080",
         "608",
         "[24816, 25674, 26532, 27390, 28248, 29106]",
         "[{'speaker': 'speaker', 'startTimeMs': 24080, 'endTimeMs': 29120, 'text': 'What we liked about cognitive services offerings were that they had a much higher fidelity.', 'confidence': 1, 'words': [], 'locale': 'en-US'}, {'speaker': 'speaker', 'startTimeMs': 29600, 'endTimeMs': 32880, 'text': 'And they sounded a lot more like an actual human voice.', 'confidence': 1, 'words': [], 'locale': 'en-US'}]"
        ],
        [
         "10",
         "# Shot 00:30.033 => 00:33.200\n## Transcript\n```\nWEBVTT\n\n00:29.600 --> 00:32.880\n<v Speaker>And they sounded a lot more like an actual human voice.\n```\n## Key Frames\n- 00:30.822 ![](keyFrame.30822.jpg)\n- 00:31.614 ![](keyFrame.31614.jpg)\n- 00:32.406 ![](keyFrame.32406.jpg)",
         "{}",
         "audioVisual",
         "30033",
         "33200",
         "1080",
         "608",
         "[30822, 31614, 32406]",
         "[{'speaker': 'speaker', 'startTimeMs': 29600, 'endTimeMs': 32880, 'text': 'And they sounded a lot more like an actual human voice.', 'confidence': 1, 'words': [], 'locale': 'en-US'}]"
        ],
        [
         "11",
         "# Shot 00:33.200 => 00:35.267\n## Transcript\n```\nWEBVTT\n\n00:33.720 --> 00:37.080\n<v Speaker>Orlando ground 9555 requesting the end of pushback.\n```\n## Key Frames\n- 00:33.891 ![](keyFrame.33891.jpg)\n- 00:34.584 ![](keyFrame.34584.jpg)",
         "{}",
         "audioVisual",
         "33200",
         "35267",
         "1080",
         "608",
         "[33891, 34584]",
         "[{'speaker': 'speaker', 'startTimeMs': 33720, 'endTimeMs': 37080, 'text': 'Orlando ground 9555 requesting the end of pushback.', 'confidence': 1, 'words': [], 'locale': 'en-US'}]"
        ],
        [
         "12",
         "# Shot 00:35.267 => 00:37.700\n## Transcript\n```\nWEBVTT\n\n00:33.720 --> 00:37.080\n<v Speaker>Orlando ground 9555 requesting the end of pushback.\n```\n## Key Frames\n- 00:36.069 ![](keyFrame.36069.jpg)\n- 00:36.861 ![](keyFrame.36861.jpg)",
         "{}",
         "audioVisual",
         "35267",
         "37700",
         "1080",
         "608",
         "[36069, 36861]",
         "[{'speaker': 'speaker', 'startTimeMs': 33720, 'endTimeMs': 37080, 'text': 'Orlando ground 9555 requesting the end of pushback.', 'confidence': 1, 'words': [], 'locale': 'en-US'}]"
        ],
        [
         "13",
         "# Shot 00:37.700 => 00:39.200\n## Transcript\n```\nWEBVTT\n\n00:38.800 --> 00:41.160\n<v Speaker>9555 request to end pushback received.\n```\n## Key Frames\n- 00:38.181 ![](keyFrame.38181.jpg)\n- 00:38.676 ![](keyFrame.38676.jpg)",
         "{}",
         "audioVisual",
         "37700",
         "39200",
         "1080",
         "608",
         "[38181, 38676]",
         "[{'speaker': 'speaker', 'startTimeMs': 38800, 'endTimeMs': 41160, 'text': '9555 request to end pushback received.', 'confidence': 1, 'words': [], 'locale': 'en-US'}]"
        ],
        [
         "14",
         "# Shot 00:39.200 => 00:42.033\n## Transcript\n```\nWEBVTT\n\n00:38.800 --> 00:41.160\n<v Speaker>9555 request to end pushback received.\n```\n## Key Frames\n- 00:39.897 ![](keyFrame.39897.jpg)\n- 00:40.590 ![](keyFrame.40590.jpg)\n- 00:41.283 ![](keyFrame.41283.jpg)",
         "{}",
         "audioVisual",
         "39200",
         "42033",
         "1080",
         "608",
         "[39897, 40590, 41283]",
         "[{'speaker': 'speaker', 'startTimeMs': 38800, 'endTimeMs': 41160, 'text': '9555 request to end pushback received.', 'confidence': 1, 'words': [], 'locale': 'en-US'}]"
        ],
        [
         "15",
         "# Shot 00:42.033 => 00:43.866\n## Transcript\n```\nWEBVTT\n\n```\n## Key Frames\n- 00:42.636 ![](keyFrame.42636.jpg)\n- 00:43.230 ![](keyFrame.43230.jpg)",
         "{}",
         "audioVisual",
         "42033",
         "43866",
         "1080",
         "608",
         "[42636, 43230]",
         "[]"
        ]
       ],
       "shape": {
        "columns": 9,
        "rows": 16
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>markdown</th>\n",
       "      <th>fields</th>\n",
       "      <th>kind</th>\n",
       "      <th>startTimeMs</th>\n",
       "      <th>endTimeMs</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>KeyFrameTimesMs</th>\n",
       "      <th>transcriptPhrases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td># Shot 00:00.000 =&gt; 00:01.467\\n## Transcript\\n...</td>\n",
       "      <td>{}</td>\n",
       "      <td>audioVisual</td>\n",
       "      <td>0</td>\n",
       "      <td>1467</td>\n",
       "      <td>1080</td>\n",
       "      <td>608</td>\n",
       "      <td>[726]</td>\n",
       "      <td>[{'speaker': 'speaker', 'startTimeMs': 1400, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td># Shot 00:01.467 =&gt; 00:03.233\\n## Transcript\\n...</td>\n",
       "      <td>{}</td>\n",
       "      <td>audioVisual</td>\n",
       "      <td>1467</td>\n",
       "      <td>3233</td>\n",
       "      <td>1080</td>\n",
       "      <td>608</td>\n",
       "      <td>[2046, 2640]</td>\n",
       "      <td>[{'speaker': 'speaker', 'startTimeMs': 1400, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td># Shot 00:03.233 =&gt; 00:07.367\\n## Transcript\\n...</td>\n",
       "      <td>{}</td>\n",
       "      <td>audioVisual</td>\n",
       "      <td>3233</td>\n",
       "      <td>7367</td>\n",
       "      <td>1080</td>\n",
       "      <td>608</td>\n",
       "      <td>[4059, 4884, 5709, 6534]</td>\n",
       "      <td>[{'speaker': 'speaker', 'startTimeMs': 1400, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td># Shot 00:07.367 =&gt; 00:08.200\\n## Transcript\\n...</td>\n",
       "      <td>{}</td>\n",
       "      <td>audioVisual</td>\n",
       "      <td>7367</td>\n",
       "      <td>8200</td>\n",
       "      <td>1080</td>\n",
       "      <td>608</td>\n",
       "      <td>[7788]</td>\n",
       "      <td>[{'speaker': 'speaker', 'startTimeMs': 7600, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td># Shot 00:08.200 =&gt; 00:11.367\\n## Transcript\\n...</td>\n",
       "      <td>{}</td>\n",
       "      <td>audioVisual</td>\n",
       "      <td>8200</td>\n",
       "      <td>11367</td>\n",
       "      <td>1080</td>\n",
       "      <td>608</td>\n",
       "      <td>[8976, 9768, 10560]</td>\n",
       "      <td>[{'speaker': 'speaker', 'startTimeMs': 7600, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td># Shot 00:11.367 =&gt; 00:13.567\\n## Transcript\\n...</td>\n",
       "      <td>{}</td>\n",
       "      <td>audioVisual</td>\n",
       "      <td>11367</td>\n",
       "      <td>13567</td>\n",
       "      <td>1080</td>\n",
       "      <td>608</td>\n",
       "      <td>[12078, 12804]</td>\n",
       "      <td>[{'speaker': 'speaker', 'startTimeMs': 7600, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td># Shot 00:13.567 =&gt; 00:16.100\\n## Transcript\\n...</td>\n",
       "      <td>{}</td>\n",
       "      <td>audioVisual</td>\n",
       "      <td>13567</td>\n",
       "      <td>16100</td>\n",
       "      <td>1080</td>\n",
       "      <td>608</td>\n",
       "      <td>[14190, 14817, 15444]</td>\n",
       "      <td>[{'speaker': 'speaker', 'startTimeMs': 13440, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td># Shot 00:16.100 =&gt; 00:19.433\\n## Transcript\\n...</td>\n",
       "      <td>{}</td>\n",
       "      <td>audioVisual</td>\n",
       "      <td>16100</td>\n",
       "      <td>19433</td>\n",
       "      <td>1080</td>\n",
       "      <td>608</td>\n",
       "      <td>[16929, 17754, 18579]</td>\n",
       "      <td>[{'speaker': 'speaker', 'startTimeMs': 13440, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td># Shot 00:19.433 =&gt; 00:23.967\\n## Transcript\\n...</td>\n",
       "      <td>{}</td>\n",
       "      <td>audioVisual</td>\n",
       "      <td>19433</td>\n",
       "      <td>23967</td>\n",
       "      <td>1080</td>\n",
       "      <td>608</td>\n",
       "      <td>[20196, 20955, 21714, 22473, 23232]</td>\n",
       "      <td>[{'speaker': 'speaker', 'startTimeMs': 13440, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td># Shot 00:23.967 =&gt; 00:30.033\\n## Transcript\\n...</td>\n",
       "      <td>{}</td>\n",
       "      <td>audioVisual</td>\n",
       "      <td>23967</td>\n",
       "      <td>30033</td>\n",
       "      <td>1080</td>\n",
       "      <td>608</td>\n",
       "      <td>[24816, 25674, 26532, 27390, 28248, 29106]</td>\n",
       "      <td>[{'speaker': 'speaker', 'startTimeMs': 24080, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td># Shot 00:30.033 =&gt; 00:33.200\\n## Transcript\\n...</td>\n",
       "      <td>{}</td>\n",
       "      <td>audioVisual</td>\n",
       "      <td>30033</td>\n",
       "      <td>33200</td>\n",
       "      <td>1080</td>\n",
       "      <td>608</td>\n",
       "      <td>[30822, 31614, 32406]</td>\n",
       "      <td>[{'speaker': 'speaker', 'startTimeMs': 29600, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td># Shot 00:33.200 =&gt; 00:35.267\\n## Transcript\\n...</td>\n",
       "      <td>{}</td>\n",
       "      <td>audioVisual</td>\n",
       "      <td>33200</td>\n",
       "      <td>35267</td>\n",
       "      <td>1080</td>\n",
       "      <td>608</td>\n",
       "      <td>[33891, 34584]</td>\n",
       "      <td>[{'speaker': 'speaker', 'startTimeMs': 33720, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td># Shot 00:35.267 =&gt; 00:37.700\\n## Transcript\\n...</td>\n",
       "      <td>{}</td>\n",
       "      <td>audioVisual</td>\n",
       "      <td>35267</td>\n",
       "      <td>37700</td>\n",
       "      <td>1080</td>\n",
       "      <td>608</td>\n",
       "      <td>[36069, 36861]</td>\n",
       "      <td>[{'speaker': 'speaker', 'startTimeMs': 33720, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td># Shot 00:37.700 =&gt; 00:39.200\\n## Transcript\\n...</td>\n",
       "      <td>{}</td>\n",
       "      <td>audioVisual</td>\n",
       "      <td>37700</td>\n",
       "      <td>39200</td>\n",
       "      <td>1080</td>\n",
       "      <td>608</td>\n",
       "      <td>[38181, 38676]</td>\n",
       "      <td>[{'speaker': 'speaker', 'startTimeMs': 38800, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td># Shot 00:39.200 =&gt; 00:42.033\\n## Transcript\\n...</td>\n",
       "      <td>{}</td>\n",
       "      <td>audioVisual</td>\n",
       "      <td>39200</td>\n",
       "      <td>42033</td>\n",
       "      <td>1080</td>\n",
       "      <td>608</td>\n",
       "      <td>[39897, 40590, 41283]</td>\n",
       "      <td>[{'speaker': 'speaker', 'startTimeMs': 38800, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td># Shot 00:42.033 =&gt; 00:43.866\\n## Transcript\\n...</td>\n",
       "      <td>{}</td>\n",
       "      <td>audioVisual</td>\n",
       "      <td>42033</td>\n",
       "      <td>43866</td>\n",
       "      <td>1080</td>\n",
       "      <td>608</td>\n",
       "      <td>[42636, 43230]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             markdown fields         kind  \\\n",
       "0   # Shot 00:00.000 => 00:01.467\\n## Transcript\\n...     {}  audioVisual   \n",
       "1   # Shot 00:01.467 => 00:03.233\\n## Transcript\\n...     {}  audioVisual   \n",
       "2   # Shot 00:03.233 => 00:07.367\\n## Transcript\\n...     {}  audioVisual   \n",
       "3   # Shot 00:07.367 => 00:08.200\\n## Transcript\\n...     {}  audioVisual   \n",
       "4   # Shot 00:08.200 => 00:11.367\\n## Transcript\\n...     {}  audioVisual   \n",
       "5   # Shot 00:11.367 => 00:13.567\\n## Transcript\\n...     {}  audioVisual   \n",
       "6   # Shot 00:13.567 => 00:16.100\\n## Transcript\\n...     {}  audioVisual   \n",
       "7   # Shot 00:16.100 => 00:19.433\\n## Transcript\\n...     {}  audioVisual   \n",
       "8   # Shot 00:19.433 => 00:23.967\\n## Transcript\\n...     {}  audioVisual   \n",
       "9   # Shot 00:23.967 => 00:30.033\\n## Transcript\\n...     {}  audioVisual   \n",
       "10  # Shot 00:30.033 => 00:33.200\\n## Transcript\\n...     {}  audioVisual   \n",
       "11  # Shot 00:33.200 => 00:35.267\\n## Transcript\\n...     {}  audioVisual   \n",
       "12  # Shot 00:35.267 => 00:37.700\\n## Transcript\\n...     {}  audioVisual   \n",
       "13  # Shot 00:37.700 => 00:39.200\\n## Transcript\\n...     {}  audioVisual   \n",
       "14  # Shot 00:39.200 => 00:42.033\\n## Transcript\\n...     {}  audioVisual   \n",
       "15  # Shot 00:42.033 => 00:43.866\\n## Transcript\\n...     {}  audioVisual   \n",
       "\n",
       "    startTimeMs  endTimeMs  width  height  \\\n",
       "0             0       1467   1080     608   \n",
       "1          1467       3233   1080     608   \n",
       "2          3233       7367   1080     608   \n",
       "3          7367       8200   1080     608   \n",
       "4          8200      11367   1080     608   \n",
       "5         11367      13567   1080     608   \n",
       "6         13567      16100   1080     608   \n",
       "7         16100      19433   1080     608   \n",
       "8         19433      23967   1080     608   \n",
       "9         23967      30033   1080     608   \n",
       "10        30033      33200   1080     608   \n",
       "11        33200      35267   1080     608   \n",
       "12        35267      37700   1080     608   \n",
       "13        37700      39200   1080     608   \n",
       "14        39200      42033   1080     608   \n",
       "15        42033      43866   1080     608   \n",
       "\n",
       "                               KeyFrameTimesMs  \\\n",
       "0                                        [726]   \n",
       "1                                 [2046, 2640]   \n",
       "2                     [4059, 4884, 5709, 6534]   \n",
       "3                                       [7788]   \n",
       "4                          [8976, 9768, 10560]   \n",
       "5                               [12078, 12804]   \n",
       "6                        [14190, 14817, 15444]   \n",
       "7                        [16929, 17754, 18579]   \n",
       "8          [20196, 20955, 21714, 22473, 23232]   \n",
       "9   [24816, 25674, 26532, 27390, 28248, 29106]   \n",
       "10                       [30822, 31614, 32406]   \n",
       "11                              [33891, 34584]   \n",
       "12                              [36069, 36861]   \n",
       "13                              [38181, 38676]   \n",
       "14                       [39897, 40590, 41283]   \n",
       "15                              [42636, 43230]   \n",
       "\n",
       "                                    transcriptPhrases  \n",
       "0   [{'speaker': 'speaker', 'startTimeMs': 1400, '...  \n",
       "1   [{'speaker': 'speaker', 'startTimeMs': 1400, '...  \n",
       "2   [{'speaker': 'speaker', 'startTimeMs': 1400, '...  \n",
       "3   [{'speaker': 'speaker', 'startTimeMs': 7600, '...  \n",
       "4   [{'speaker': 'speaker', 'startTimeMs': 7600, '...  \n",
       "5   [{'speaker': 'speaker', 'startTimeMs': 7600, '...  \n",
       "6   [{'speaker': 'speaker', 'startTimeMs': 13440, ...  \n",
       "7   [{'speaker': 'speaker', 'startTimeMs': 13440, ...  \n",
       "8   [{'speaker': 'speaker', 'startTimeMs': 13440, ...  \n",
       "9   [{'speaker': 'speaker', 'startTimeMs': 24080, ...  \n",
       "10  [{'speaker': 'speaker', 'startTimeMs': 29600, ...  \n",
       "11  [{'speaker': 'speaker', 'startTimeMs': 33720, ...  \n",
       "12  [{'speaker': 'speaker', 'startTimeMs': 33720, ...  \n",
       "13  [{'speaker': 'speaker', 'startTimeMs': 38800, ...  \n",
       "14  [{'speaker': 'speaker', 'startTimeMs': 38800, ...  \n",
       "15                                                 []  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df_temp = result_df.copy()\n",
    "\n",
    "result_df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df_temp['markdown'] = result_df_temp['markdown'].str.replace(\"![](\", \"![](../data/key_frames/\", case=False, regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Shot 00:00.000 => 00:01.467\n",
       "## Transcript\n",
       "```\n",
       "WEBVTT\n",
       "\n",
       "00:01.400 --> 00:06.560\n",
       "<v Speaker>When it comes to the neural TTS, in order to get a good voice, it's better to have good data.\n",
       "```\n",
       "## Key Frames\n",
       "- 00:00.726 ![](../data/key_frames/keyFrame.726.jpg)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# Shot 00:01.467 => 00:03.233\n",
       "## Transcript\n",
       "```\n",
       "WEBVTT\n",
       "\n",
       "00:01.400 --> 00:06.560\n",
       "<v Speaker>When it comes to the neural TTS, in order to get a good voice, it's better to have good data.\n",
       "```\n",
       "## Key Frames\n",
       "- 00:02.046 ![](../data/key_frames/keyFrame.2046.jpg)\n",
       "- 00:02.640 ![](../data/key_frames/keyFrame.2640.jpg)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# Shot 00:03.233 => 00:07.367\n",
       "## Transcript\n",
       "```\n",
       "WEBVTT\n",
       "\n",
       "00:01.400 --> 00:06.560\n",
       "<v Speaker>When it comes to the neural TTS, in order to get a good voice, it's better to have good data.\n",
       "```\n",
       "## Key Frames\n",
       "- 00:04.059 ![](../data/key_frames/keyFrame.4059.jpg)\n",
       "- 00:04.884 ![](../data/key_frames/keyFrame.4884.jpg)\n",
       "- 00:05.709 ![](../data/key_frames/keyFrame.5709.jpg)\n",
       "- 00:06.534 ![](../data/key_frames/keyFrame.6534.jpg)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# Shot 00:07.367 => 00:08.200\n",
       "## Transcript\n",
       "```\n",
       "WEBVTT\n",
       "\n",
       "00:07.600 --> 00:13.320\n",
       "<v Speaker>To achieve that, we build a universal TTS model based on 3,000 hours of data.\n",
       "```\n",
       "## Key Frames\n",
       "- 00:07.788 ![](../data/key_frames/keyFrame.7788.jpg)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# Shot 00:08.200 => 00:11.367\n",
       "## Transcript\n",
       "```\n",
       "WEBVTT\n",
       "\n",
       "00:07.600 --> 00:13.320\n",
       "<v Speaker>To achieve that, we build a universal TTS model based on 3,000 hours of data.\n",
       "```\n",
       "## Key Frames\n",
       "- 00:08.976 ![](../data/key_frames/keyFrame.8976.jpg)\n",
       "- 00:09.768 ![](../data/key_frames/keyFrame.9768.jpg)\n",
       "- 00:10.560 ![](../data/key_frames/keyFrame.10560.jpg)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# Shot 00:11.367 => 00:13.567\n",
       "## Transcript\n",
       "```\n",
       "WEBVTT\n",
       "\n",
       "00:07.600 --> 00:13.320\n",
       "<v Speaker>To achieve that, we build a universal TTS model based on 3,000 hours of data.\n",
       "00:13.440 --> 00:23.640\n",
       "<v Speaker>We actually accumulated tons of the data so that this universal model is able to capture the nuance of the audio and generate a more natural voice for the algorithm.\n",
       "```\n",
       "## Key Frames\n",
       "- 00:12.078 ![](../data/key_frames/keyFrame.12078.jpg)\n",
       "- 00:12.804 ![](../data/key_frames/keyFrame.12804.jpg)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# Shot 00:13.567 => 00:16.100\n",
       "## Transcript\n",
       "```\n",
       "WEBVTT\n",
       "\n",
       "00:13.440 --> 00:23.640\n",
       "<v Speaker>We actually accumulated tons of the data so that this universal model is able to capture the nuance of the audio and generate a more natural voice for the algorithm.\n",
       "```\n",
       "## Key Frames\n",
       "- 00:14.190 ![](../data/key_frames/keyFrame.14190.jpg)\n",
       "- 00:14.817 ![](../data/key_frames/keyFrame.14817.jpg)\n",
       "- 00:15.444 ![](../data/key_frames/keyFrame.15444.jpg)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# Shot 00:16.100 => 00:19.433\n",
       "## Transcript\n",
       "```\n",
       "WEBVTT\n",
       "\n",
       "00:13.440 --> 00:23.640\n",
       "<v Speaker>We actually accumulated tons of the data so that this universal model is able to capture the nuance of the audio and generate a more natural voice for the algorithm.\n",
       "```\n",
       "## Key Frames\n",
       "- 00:16.929 ![](../data/key_frames/keyFrame.16929.jpg)\n",
       "- 00:17.754 ![](../data/key_frames/keyFrame.17754.jpg)\n",
       "- 00:18.579 ![](../data/key_frames/keyFrame.18579.jpg)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# Shot 00:19.433 => 00:23.967\n",
       "## Transcript\n",
       "```\n",
       "WEBVTT\n",
       "\n",
       "00:13.440 --> 00:23.640\n",
       "<v Speaker>We actually accumulated tons of the data so that this universal model is able to capture the nuance of the audio and generate a more natural voice for the algorithm.\n",
       "```\n",
       "## Key Frames\n",
       "- 00:20.196 ![](../data/key_frames/keyFrame.20196.jpg)\n",
       "- 00:20.955 ![](../data/key_frames/keyFrame.20955.jpg)\n",
       "- 00:21.714 ![](../data/key_frames/keyFrame.21714.jpg)\n",
       "- 00:22.473 ![](../data/key_frames/keyFrame.22473.jpg)\n",
       "- 00:23.232 ![](../data/key_frames/keyFrame.23232.jpg)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# Shot 00:23.967 => 00:30.033\n",
       "## Transcript\n",
       "```\n",
       "WEBVTT\n",
       "\n",
       "00:24.080 --> 00:29.120\n",
       "<v Speaker>What we liked about cognitive services offerings were that they had a much higher fidelity.\n",
       "00:29.600 --> 00:32.880\n",
       "<v Speaker>And they sounded a lot more like an actual human voice.\n",
       "```\n",
       "## Key Frames\n",
       "- 00:24.816 ![](../data/key_frames/keyFrame.24816.jpg)\n",
       "- 00:25.674 ![](../data/key_frames/keyFrame.25674.jpg)\n",
       "- 00:26.532 ![](../data/key_frames/keyFrame.26532.jpg)\n",
       "- 00:27.390 ![](../data/key_frames/keyFrame.27390.jpg)\n",
       "- 00:28.248 ![](../data/key_frames/keyFrame.28248.jpg)\n",
       "- 00:29.106 ![](../data/key_frames/keyFrame.29106.jpg)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# Shot 00:30.033 => 00:33.200\n",
       "## Transcript\n",
       "```\n",
       "WEBVTT\n",
       "\n",
       "00:29.600 --> 00:32.880\n",
       "<v Speaker>And they sounded a lot more like an actual human voice.\n",
       "```\n",
       "## Key Frames\n",
       "- 00:30.822 ![](../data/key_frames/keyFrame.30822.jpg)\n",
       "- 00:31.614 ![](../data/key_frames/keyFrame.31614.jpg)\n",
       "- 00:32.406 ![](../data/key_frames/keyFrame.32406.jpg)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# Shot 00:33.200 => 00:35.267\n",
       "## Transcript\n",
       "```\n",
       "WEBVTT\n",
       "\n",
       "00:33.720 --> 00:37.080\n",
       "<v Speaker>Orlando ground 9555 requesting the end of pushback.\n",
       "```\n",
       "## Key Frames\n",
       "- 00:33.891 ![](../data/key_frames/keyFrame.33891.jpg)\n",
       "- 00:34.584 ![](../data/key_frames/keyFrame.34584.jpg)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# Shot 00:35.267 => 00:37.700\n",
       "## Transcript\n",
       "```\n",
       "WEBVTT\n",
       "\n",
       "00:33.720 --> 00:37.080\n",
       "<v Speaker>Orlando ground 9555 requesting the end of pushback.\n",
       "```\n",
       "## Key Frames\n",
       "- 00:36.069 ![](../data/key_frames/keyFrame.36069.jpg)\n",
       "- 00:36.861 ![](../data/key_frames/keyFrame.36861.jpg)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# Shot 00:37.700 => 00:39.200\n",
       "## Transcript\n",
       "```\n",
       "WEBVTT\n",
       "\n",
       "00:38.800 --> 00:41.160\n",
       "<v Speaker>9555 request to end pushback received.\n",
       "```\n",
       "## Key Frames\n",
       "- 00:38.181 ![](../data/key_frames/keyFrame.38181.jpg)\n",
       "- 00:38.676 ![](../data/key_frames/keyFrame.38676.jpg)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# Shot 00:39.200 => 00:42.033\n",
       "## Transcript\n",
       "```\n",
       "WEBVTT\n",
       "\n",
       "00:38.800 --> 00:41.160\n",
       "<v Speaker>9555 request to end pushback received.\n",
       "```\n",
       "## Key Frames\n",
       "- 00:39.897 ![](../data/key_frames/keyFrame.39897.jpg)\n",
       "- 00:40.590 ![](../data/key_frames/keyFrame.40590.jpg)\n",
       "- 00:41.283 ![](../data/key_frames/keyFrame.41283.jpg)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# Shot 00:42.033 => 00:43.866\n",
       "## Transcript\n",
       "```\n",
       "WEBVTT\n",
       "\n",
       "```\n",
       "## Key Frames\n",
       "- 00:42.636 ![](../data/key_frames/keyFrame.42636.jpg)\n",
       "- 00:43.230 ![](../data/key_frames/keyFrame.43230.jpg)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "target_column = \"markdown\"\n",
    "\n",
    "for index, row in result_df_temp.iterrows():\n",
    "    content = row[target_column]\n",
    "    if isinstance(content, list):\n",
    "        for item in content:\n",
    "            display(Markdown(str(item)))\n",
    "    else:\n",
    "        display(Markdown(str(content)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:python.content_understanding_client:Analyzer content-video-sample-8d1242a6-cbe0-4414-af65-b53fee8ed15c deleted.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Response [204]>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.delete_analyzer(ANALYZER_ID)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
